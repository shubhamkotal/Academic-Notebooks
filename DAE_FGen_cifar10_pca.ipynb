{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE - Denoising Auto-Encoder\n",
    "\n",
    "In this auto-encoder, we corrupt the input by slightly adding noise to it and train the network to reconstruct the original input. This can be achieved in multiple ways. \n",
    "1. Add dropout to the input. This will randomly turn off few inputs, which acts as noise. (We'll use this)\n",
    "2. Add gaussian or uniform noise to the input\n",
    "\n",
    "We'll see how classification performance varies with respect to RAW Vs. Encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 781)\n",
      "(10000, 10)\n",
      "(10000, 781)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"D:\\insofe\\AI\\cifar_autoencoders\")\n",
    "\n",
    "# Import the necessary libraries/modules\n",
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n",
    "\n",
    "# We'll use only 10,000 out of 50,000 samples for this \n",
    "tmp = np.load('cifar_pca_train.npz') # '.npz' is a dictionary which can hold many arrays\n",
    "train_data = tmp['data'][:10000]     # 'data' holds the train data\n",
    "train_labels = tmp['labels'][:10000] # 'labels' hold the corresponding labels for the above data\n",
    "\n",
    "tmp = np.load('cifar_pca_test.npz')\n",
    "test_data = tmp['data']\n",
    "test_labels = tmp['labels']\n",
    "\n",
    "\n",
    "# Converting labels into one-hot vectors for training. one-hot encoding is nothing but dummyfing\n",
    "train_labels = to_categorical(train_labels, 10) \n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 39s 4ms/step - loss: 2.1393 - acc: 0.2296 - val_loss: 1.8586 - val_acc: 0.3511\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.9174 - acc: 0.3095 - val_loss: 1.8297 - val_acc: 0.3312\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.8437 - acc: 0.3431 - val_loss: 1.7751 - val_acc: 0.3748\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.7834 - acc: 0.3622 - val_loss: 1.7659 - val_acc: 0.3709\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 35s 4ms/step - loss: 1.7480 - acc: 0.3805 - val_loss: 1.7335 - val_acc: 0.3874\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.7147 - acc: 0.4004 - val_loss: 1.7419 - val_acc: 0.3768\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 1.6824 - acc: 0.4100 - val_loss: 1.7184 - val_acc: 0.3854\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.6490 - acc: 0.4177 - val_loss: 1.7165 - val_acc: 0.3921\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 1.6331 - acc: 0.4286 - val_loss: 1.7084 - val_acc: 0.3921\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.5924 - acc: 0.4422 - val_loss: 1.6846 - val_acc: 0.3998\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.5535 - acc: 0.4556 - val_loss: 1.6910 - val_acc: 0.4068\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.5369 - acc: 0.4585 - val_loss: 1.6827 - val_acc: 0.4061\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.5105 - acc: 0.4710 - val_loss: 1.7024 - val_acc: 0.3963\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 1.4963 - acc: 0.4770 - val_loss: 1.6715 - val_acc: 0.4081\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 1.4649 - acc: 0.4873 - val_loss: 1.6764 - val_acc: 0.4087\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.4481 - acc: 0.4943 - val_loss: 1.6640 - val_acc: 0.4124\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.4327 - acc: 0.5009 - val_loss: 1.6674 - val_acc: 0.4125\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.3949 - acc: 0.5157 - val_loss: 1.6661 - val_acc: 0.4156\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.3794 - acc: 0.5152 - val_loss: 1.6795 - val_acc: 0.4116\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 1.3597 - acc: 0.5181 - val_loss: 1.6633 - val_acc: 0.4094\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.3296 - acc: 0.5372 - val_loss: 1.6796 - val_acc: 0.4090\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.3184 - acc: 0.5373 - val_loss: 1.6752 - val_acc: 0.4133\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.3004 - acc: 0.5484 - val_loss: 1.6644 - val_acc: 0.4153\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 1.2761 - acc: 0.5539 - val_loss: 1.6602 - val_acc: 0.4255\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.2488 - acc: 0.5663 - val_loss: 1.6545 - val_acc: 0.4244\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 1.2567 - acc: 0.5584 - val_loss: 1.6694 - val_acc: 0.4188\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.2187 - acc: 0.5731 - val_loss: 1.6687 - val_acc: 0.4220\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 1.2156 - acc: 0.5782 - val_loss: 1.6885 - val_acc: 0.4144\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.1849 - acc: 0.5905 - val_loss: 1.6715 - val_acc: 0.4197\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.1810 - acc: 0.5917 - val_loss: 1.6587 - val_acc: 0.4263\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.1450 - acc: 0.5979 - val_loss: 1.6896 - val_acc: 0.4249\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.1336 - acc: 0.6027 - val_loss: 1.6718 - val_acc: 0.4215\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.1358 - acc: 0.6000 - val_loss: 1.6731 - val_acc: 0.4286\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.1181 - acc: 0.6037 - val_loss: 1.6841 - val_acc: 0.4157\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 1.1017 - acc: 0.6189 - val_loss: 1.6966 - val_acc: 0.4252\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 1.0998 - acc: 0.6211 - val_loss: 1.6670 - val_acc: 0.4253\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.0796 - acc: 0.6235 - val_loss: 1.6814 - val_acc: 0.4257\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 1.0555 - acc: 0.6296 - val_loss: 1.6804 - val_acc: 0.4292\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 1.0493 - acc: 0.6340 - val_loss: 1.6809 - val_acc: 0.4234\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.0245 - acc: 0.6460 - val_loss: 1.6844 - val_acc: 0.4246\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 1.0248 - acc: 0.6447 - val_loss: 1.6894 - val_acc: 0.4327\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 1.0101 - acc: 0.6523 - val_loss: 1.7002 - val_acc: 0.4251\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 0.9877 - acc: 0.6544 - val_loss: 1.7080 - val_acc: 0.4325\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 0.9826 - acc: 0.6601 - val_loss: 1.6927 - val_acc: 0.4329\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 0.9759 - acc: 0.6640 - val_loss: 1.6937 - val_acc: 0.4322\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 0.9561 - acc: 0.6694 - val_loss: 1.6951 - val_acc: 0.4324\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 0.9521 - acc: 0.6732 - val_loss: 1.7105 - val_acc: 0.4340\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 0.9316 - acc: 0.6785 - val_loss: 1.7078 - val_acc: 0.4287\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.9283 - acc: 0.6818 - val_loss: 1.7268 - val_acc: 0.4266\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.9144 - acc: 0.6842 - val_loss: 1.7044 - val_acc: 0.4316\n"
     ]
    }
   ],
   "source": [
    "# training a two hidden layer MLP for classification task\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(781,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "history = mlp.fit(train_data, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6812 - mean_squared_error: 0.6812 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5708 - mean_squared_error: 0.5708 - val_loss: 0.2852 - val_mean_squared_error: 0.2852\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5476 - mean_squared_error: 0.5476 - val_loss: 0.2601 - val_mean_squared_error: 0.2601\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5385 - mean_squared_error: 0.5385 - val_loss: 0.2485 - val_mean_squared_error: 0.2485\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5316 - mean_squared_error: 0.5316 - val_loss: 0.2373 - val_mean_squared_error: 0.2373\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5302 - mean_squared_error: 0.5302 - val_loss: 0.2328 - val_mean_squared_error: 0.2328\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5278 - mean_squared_error: 0.5278 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5202 - mean_squared_error: 0.5202 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.2199 - val_mean_squared_error: 0.2199\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 0.2195 - val_mean_squared_error: 0.2195\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5208 - mean_squared_error: 0.5208 - val_loss: 0.2176 - val_mean_squared_error: 0.2176\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5118 - mean_squared_error: 0.5118 - val_loss: 0.2136 - val_mean_squared_error: 0.2136\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5156 - mean_squared_error: 0.5156 - val_loss: 0.2157 - val_mean_squared_error: 0.2157\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5120 - mean_squared_error: 0.5120 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5127 - mean_squared_error: 0.5127 - val_loss: 0.2103 - val_mean_squared_error: 0.2103\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5102 - mean_squared_error: 0.5102 - val_loss: 0.2104 - val_mean_squared_error: 0.2104\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5124 - mean_squared_error: 0.5124 - val_loss: 0.2098 - val_mean_squared_error: 0.2098\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5125 - mean_squared_error: 0.5125 - val_loss: 0.2080 - val_mean_squared_error: 0.2080\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5050 - mean_squared_error: 0.5050 - val_loss: 0.2066 - val_mean_squared_error: 0.2066\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5045 - mean_squared_error: 0.5045 - val_loss: 0.2048 - val_mean_squared_error: 0.2048\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5066 - mean_squared_error: 0.5066 - val_loss: 0.2052 - val_mean_squared_error: 0.2052\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5080 - mean_squared_error: 0.5080 - val_loss: 0.2050 - val_mean_squared_error: 0.2050\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5091 - mean_squared_error: 0.5091 - val_loss: 0.2025 - val_mean_squared_error: 0.2025\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5065 - mean_squared_error: 0.5065 - val_loss: 0.2058 - val_mean_squared_error: 0.2058\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5077 - mean_squared_error: 0.5077 - val_loss: 0.2017 - val_mean_squared_error: 0.2017\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5034 - mean_squared_error: 0.5034 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5049 - mean_squared_error: 0.5049 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5014 - mean_squared_error: 0.5014 - val_loss: 0.2016 - val_mean_squared_error: 0.2016\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5051 - mean_squared_error: 0.5051 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 0.1997 - val_mean_squared_error: 0.1997\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5008 - mean_squared_error: 0.5008 - val_loss: 0.2018 - val_mean_squared_error: 0.2018\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4999 - mean_squared_error: 0.4999 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5018 - mean_squared_error: 0.5018 - val_loss: 0.1999 - val_mean_squared_error: 0.1999\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4980 - mean_squared_error: 0.4980 - val_loss: 0.1990 - val_mean_squared_error: 0.1990\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4991 - mean_squared_error: 0.4991 - val_loss: 0.1988 - val_mean_squared_error: 0.1988\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4987 - mean_squared_error: 0.4987 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4976 - mean_squared_error: 0.4976 - val_loss: 0.1986 - val_mean_squared_error: 0.1986\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4966 - mean_squared_error: 0.4966 - val_loss: 0.1960 - val_mean_squared_error: 0.1960\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2006 - val_mean_squared_error: 0.2006\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4950 - mean_squared_error: 0.4950 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.2007 - val_mean_squared_error: 0.2007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4928 - mean_squared_error: 0.4928 - val_loss: 0.1991 - val_mean_squared_error: 0.1991\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4951 - mean_squared_error: 0.4951 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4943 - mean_squared_error: 0.4943 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4920 - mean_squared_error: 0.4920 - val_loss: 0.2005 - val_mean_squared_error: 0.2005\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4937 - mean_squared_error: 0.4937 - val_loss: 0.2031 - val_mean_squared_error: 0.2031\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4911 - mean_squared_error: 0.4911 - val_loss: 0.1987 - val_mean_squared_error: 0.1987\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4895 - mean_squared_error: 0.4895 - val_loss: 0.2027 - val_mean_squared_error: 0.2027\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4892 - mean_squared_error: 0.4892 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4919 - mean_squared_error: 0.4919 - val_loss: 0.2000 - val_mean_squared_error: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Building an auto-encoder architecture with 'Model' function. This is a bit defferent from 'Sequential' type.\n",
    "# For this, we need to create a series of layers connected together. \n",
    "# Once we have the connections in place, we can use model to define the architecture.\n",
    "# To 'Model', we simply mention the first layer and the last layer.\n",
    "\n",
    "nb_epoch = 50      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "\n",
    "input_img = Input(shape=(781,))\n",
    "crrpt_img = Dropout(0.5)(input_img)\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img)\n",
    "decoded = Dense(781, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error')\n",
    "\n",
    "history = autoencoder.fit(train_data, train_data,  \n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data))\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5') # save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting encoded output, we can define a model which starts with input layer and ends with encoded layer\n",
    "# Since these layers are already trained, we can directly predict the encoded values\n",
    "\n",
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for classifying cifar data using encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 1000), (10000, 10))\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s - loss: 2.0458 - acc: 0.2645 - val_loss: 1.8417 - val_acc: 0.3386\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.8226 - acc: 0.3504 - val_loss: 1.7803 - val_acc: 0.3662\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7520 - acc: 0.3779 - val_loss: 1.7306 - val_acc: 0.3822\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.7034 - acc: 0.3979 - val_loss: 1.7332 - val_acc: 0.3840\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6570 - acc: 0.4159 - val_loss: 1.7385 - val_acc: 0.3847\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.6137 - acc: 0.4310 - val_loss: 1.6896 - val_acc: 0.3938\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5799 - acc: 0.4465 - val_loss: 1.6942 - val_acc: 0.3964\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5386 - acc: 0.4622 - val_loss: 1.6447 - val_acc: 0.4174\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.5048 - acc: 0.4618 - val_loss: 1.6501 - val_acc: 0.4198\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4584 - acc: 0.4867 - val_loss: 1.6402 - val_acc: 0.4176\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.4250 - acc: 0.4964 - val_loss: 1.6087 - val_acc: 0.4316\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3852 - acc: 0.5178 - val_loss: 1.5805 - val_acc: 0.4402\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.3638 - acc: 0.5159 - val_loss: 1.5731 - val_acc: 0.4440\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.3337 - acc: 0.5321 - val_loss: 1.5744 - val_acc: 0.4428\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2842 - acc: 0.5460 - val_loss: 1.5532 - val_acc: 0.4496\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2638 - acc: 0.5548 - val_loss: 1.5700 - val_acc: 0.4390\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.2241 - acc: 0.5727 - val_loss: 1.5412 - val_acc: 0.4536\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1910 - acc: 0.5780 - val_loss: 1.5579 - val_acc: 0.4559\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1617 - acc: 0.5912 - val_loss: 1.5380 - val_acc: 0.4582\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.1252 - acc: 0.6086 - val_loss: 1.5615 - val_acc: 0.4450\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0839 - acc: 0.6261 - val_loss: 1.5274 - val_acc: 0.4622\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0612 - acc: 0.6300 - val_loss: 1.5382 - val_acc: 0.4605\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 1s - loss: 1.0118 - acc: 0.6490 - val_loss: 1.5605 - val_acc: 0.4566\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9968 - acc: 0.6520 - val_loss: 1.5487 - val_acc: 0.4603\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9607 - acc: 0.6638 - val_loss: 1.5449 - val_acc: 0.4588\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.9352 - acc: 0.6753 - val_loss: 1.5458 - val_acc: 0.4653\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8904 - acc: 0.6944 - val_loss: 1.5516 - val_acc: 0.4673\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8723 - acc: 0.6963 - val_loss: 1.5421 - val_acc: 0.4722\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8553 - acc: 0.7060 - val_loss: 1.5706 - val_acc: 0.4606\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.8215 - acc: 0.7183 - val_loss: 1.5472 - val_acc: 0.4642\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7995 - acc: 0.7258 - val_loss: 1.5365 - val_acc: 0.4707\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7662 - acc: 0.7403 - val_loss: 1.5651 - val_acc: 0.4662\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7438 - acc: 0.7387 - val_loss: 1.5625 - val_acc: 0.4690\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.7140 - acc: 0.7551 - val_loss: 1.5673 - val_acc: 0.4669\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6890 - acc: 0.7696 - val_loss: 1.5661 - val_acc: 0.4694\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6730 - acc: 0.7749 - val_loss: 1.5720 - val_acc: 0.4694\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6458 - acc: 0.7808 - val_loss: 1.5906 - val_acc: 0.4679\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.6245 - acc: 0.7882 - val_loss: 1.5689 - val_acc: 0.4714\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5999 - acc: 0.7960 - val_loss: 1.6138 - val_acc: 0.4701\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5965 - acc: 0.7965 - val_loss: 1.5817 - val_acc: 0.4743\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5653 - acc: 0.8093 - val_loss: 1.5862 - val_acc: 0.4731\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5465 - acc: 0.8180 - val_loss: 1.5876 - val_acc: 0.4708\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5513 - acc: 0.8123 - val_loss: 1.6136 - val_acc: 0.4667\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5169 - acc: 0.8254 - val_loss: 1.5900 - val_acc: 0.4754\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.5089 - acc: 0.8302 - val_loss: 1.6528 - val_acc: 0.4679\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4888 - acc: 0.8346 - val_loss: 1.6172 - val_acc: 0.4732\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4744 - acc: 0.8406 - val_loss: 1.6775 - val_acc: 0.4684\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4762 - acc: 0.8368 - val_loss: 1.6430 - val_acc: 0.4710\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4518 - acc: 0.8449 - val_loss: 1.6464 - val_acc: 0.4748\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 1s - loss: 0.4394 - acc: 0.8519 - val_loss: 1.6689 - val_acc: 0.4716\n"
     ]
    }
   ],
   "source": [
    "# training an MLP on autoencoder features\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(1000,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "print(htest_data.shape, test_labels.shape)\n",
    "history = mlp.fit(htrain_data[:10000], train_labels[:10000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(htest_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
