{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time flies like an arrow', 'Fruit flies like a banana,', 'Sam sat on the cat', 'The cat is white.']\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = '''Time flies like an arrow\n",
    "Fruit flies like a banana,\n",
    "Sam sat on the cat\n",
    "The cat is white.'''\n",
    "\n",
    "t0 = time()\n",
    "dataset = data.split('\\n')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features...\n",
      "done in 0.006s.\n"
     ]
    }
   ],
   "source": [
    "#Get TF-IDFs.\n",
    "print(\"Extracting tf-idf features...\")\n",
    "#First we initiate an empty tfidf object with specific conditions\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))#max_df=0.95, min_df=2, stop_words='english' #USE HELP TO SEE WHAT EACH DOES)\n",
    "t0 = time()\n",
    "#Next we give the data for processing\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 32)\t0.30338183323942114\n",
      "  (0, 7)\t0.23918972200786026\n",
      "  (0, 16)\t0.23918972200786026\n",
      "  (0, 0)\t0.30338183323942114\n",
      "  (0, 2)\t0.30338183323942114\n",
      "  (0, 33)\t0.30338183323942114\n",
      "  (0, 8)\t0.23918972200786026\n",
      "  (0, 17)\t0.30338183323942114\n",
      "  (0, 1)\t0.30338183323942114\n",
      "  (0, 34)\t0.30338183323942114\n",
      "  (0, 9)\t0.30338183323942114\n",
      "  (0, 18)\t0.30338183323942114\n",
      "  (1, 7)\t0.2811316284405006\n",
      "  (1, 16)\t0.2811316284405006\n",
      "  (1, 8)\t0.2811316284405006\n",
      "  (1, 11)\t0.3565798233381452\n",
      "  (1, 3)\t0.3565798233381452\n",
      "  (1, 12)\t0.3565798233381452\n",
      "  (1, 19)\t0.3565798233381452\n",
      "  (1, 13)\t0.3565798233381452\n",
      "  (1, 10)\t0.3565798233381452\n",
      "  (2, 23)\t0.30338183323942114\n",
      "  (2, 26)\t0.30338183323942114\n",
      "  (2, 20)\t0.30338183323942114\n",
      "  (2, 29)\t0.23918972200786026\n",
      "  (2, 4)\t0.23918972200786026\n",
      "  (2, 24)\t0.30338183323942114\n",
      "  (2, 27)\t0.30338183323942114\n",
      "  (2, 21)\t0.30338183323942114\n",
      "  (2, 30)\t0.23918972200786026\n",
      "  (2, 25)\t0.30338183323942114\n",
      "  (2, 28)\t0.30338183323942114\n",
      "  (2, 22)\t0.30338183323942114\n",
      "  (3, 29)\t0.2811316284405006\n",
      "  (3, 4)\t0.2811316284405006\n",
      "  (3, 30)\t0.2811316284405006\n",
      "  (3, 14)\t0.3565798233381452\n",
      "  (3, 35)\t0.3565798233381452\n",
      "  (3, 5)\t0.3565798233381452\n",
      "  (3, 15)\t0.3565798233381452\n",
      "  (3, 31)\t0.3565798233381452\n",
      "  (3, 6)\t0.3565798233381452\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30338183 0.30338183 0.30338183 0.         0.         0.\n",
      "  0.         0.23918972 0.23918972 0.30338183 0.         0.\n",
      "  0.         0.         0.         0.         0.23918972 0.30338183\n",
      "  0.30338183 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30338183 0.30338183 0.30338183 0.        ]\n",
      " [0.         0.         0.         0.35657982 0.         0.\n",
      "  0.         0.28113163 0.28113163 0.         0.35657982 0.35657982\n",
      "  0.35657982 0.35657982 0.         0.         0.28113163 0.\n",
      "  0.         0.35657982 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.23918972 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30338183 0.30338183 0.30338183 0.30338183\n",
      "  0.30338183 0.30338183 0.30338183 0.30338183 0.30338183 0.23918972\n",
      "  0.23918972 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.28113163 0.35657982\n",
      "  0.35657982 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35657982 0.35657982 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28113163\n",
      "  0.28113163 0.35657982 0.         0.         0.         0.35657982]]\n"
     ]
    }
   ],
   "source": [
    "dense = tfidf.todense()\n",
    "dense.shape\n",
    "print(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'an arrow', 'arrow', 'banana', 'cat', 'cat is', 'cat is white', 'flies', 'flies like', 'flies like an', 'flies like banana', 'fruit', 'fruit flies', 'fruit flies like', 'is', 'is white', 'like', 'like an', 'like an arrow', 'like banana', 'on', 'on the', 'on the cat', 'sam', 'sam sat', 'sam sat on', 'sat', 'sat on', 'sat on the', 'the', 'the cat', 'the cat is', 'time', 'time flies', 'time flies like', 'white']\n",
      "['Time flies like an arrow', 'Fruit flies like a banana,', 'Sam sat on the cat', 'The cat is white.']\n",
      "['fruit flies like', 'is', 'is white', 'like', 'like an', 'like an arrow', 'like banana']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print((feature_names))\n",
    "print(dataset)\n",
    "print(feature_names[13:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc ranking\n",
    "\n",
    "Given a new query, how to find out which document is it closest to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = 'Time flies like Sam'\n",
    "response = tfidf_vectorizer.transform([new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We could also use KNN for finding the closest document. \n",
    "(In this case it seems a stretch to call KNN on four data points, but the idea is to introduce the syntax for how to run a machine learning algorithm in python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is how an ML algorithm is instantiated.\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "model = KNeighborsClassifier(n_neighbors=1, algorithm='brute')\n",
    "#This is how any ML algorithm is trained.\n",
    "model.fit(dense, [1,2,3,4])\n",
    "#This is how such an ML algorithm is used for prediction\n",
    "model.predict(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITY SHEET (Language Detection)\n",
    "The file 'english_german_articles.txt' is a collection of english and german wikipedia articles. \n",
    "Each line is a different article. \n",
    "For simplicity, the first 90 articles are sorted out as german and the last 90 as english articles.\n",
    "\n",
    "##### The ground truth is going to be ['ge', 'ge', 'ge', 'ge', .... 90 times, 'en', 'en', 'en', ... 90 times]\n",
    "* Use f.readlines() to load the corpus\n",
    "* Build a tfidf on the corpus. What is the size of the tfidf matrix?\n",
    "* Build a KMeans classifier on the matrix with 2 clusters and get the prediciton lables. \n",
    "\n",
    "\n",
    "```from sklearn.cluster import KMeans  \n",
    "model = KMeans(n_clusters=2, init = 'k-means++')  \n",
    "model.fit(tfidf_dense_matrix)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Get the confusion matrix\n",
    "#following is optional for your practice\n",
    "* Try out a different classifier and see how the results are\n",
    "* Split the matrix into test and train and redo the problem (Obviously, this method of identifying the language is more complete than the activity given above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity Solution (First try it out on your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6918bf614b3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating sentence tokens and printing the sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating sentence tokens and printing the sentences\n",
    "sentences = nltk.sent_tokenize(string)\n",
    "for sent in sentences:\n",
    "    print('\\n')\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
