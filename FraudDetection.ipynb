{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharg\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED =7124\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data and lookng at the structure of the data. This data has 30 attributes and 1 lakh records\n",
    "data = pd.read_csv(\"Fraud_data_amtstd.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.836500</td>\n",
       "      <td>-0.545419</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>0.537174</td>\n",
       "      <td>-0.426143</td>\n",
       "      <td>-0.100606</td>\n",
       "      <td>-0.584764</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>2.268429</td>\n",
       "      <td>-0.365185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>0.410736</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.602906</td>\n",
       "      <td>-0.350260</td>\n",
       "      <td>0.464407</td>\n",
       "      <td>-0.070917</td>\n",
       "      <td>-0.030486</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.289880</td>\n",
       "      <td>-2.576061</td>\n",
       "      <td>-0.092256</td>\n",
       "      <td>1.976405</td>\n",
       "      <td>2.810033</td>\n",
       "      <td>-2.669128</td>\n",
       "      <td>-0.981883</td>\n",
       "      <td>-0.470310</td>\n",
       "      <td>-0.025692</td>\n",
       "      <td>0.099528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473240</td>\n",
       "      <td>-0.307295</td>\n",
       "      <td>-2.789549</td>\n",
       "      <td>0.578976</td>\n",
       "      <td>-0.837979</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.353451</td>\n",
       "      <td>-1.662202</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.131318</td>\n",
       "      <td>0.139818</td>\n",
       "      <td>0.586921</td>\n",
       "      <td>1.069291</td>\n",
       "      <td>-0.334908</td>\n",
       "      <td>-0.204938</td>\n",
       "      <td>-0.135526</td>\n",
       "      <td>0.043821</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.182139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>-0.167062</td>\n",
       "      <td>-0.048054</td>\n",
       "      <td>-0.009912</td>\n",
       "      <td>0.417694</td>\n",
       "      <td>-0.479793</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>-0.208963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866956</td>\n",
       "      <td>1.373947</td>\n",
       "      <td>1.948343</td>\n",
       "      <td>2.686750</td>\n",
       "      <td>-0.366790</td>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.278349</td>\n",
       "      <td>0.739536</td>\n",
       "      <td>-1.655955</td>\n",
       "      <td>0.708396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>-0.070619</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.159131</td>\n",
       "      <td>0.157940</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.842670</td>\n",
       "      <td>1.401843</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>1.070402</td>\n",
       "      <td>0.843883</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.366716</td>\n",
       "      <td>0.616739</td>\n",
       "      <td>-1.586963</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036573</td>\n",
       "      <td>-0.182581</td>\n",
       "      <td>-0.226834</td>\n",
       "      <td>-1.029794</td>\n",
       "      <td>-0.118762</td>\n",
       "      <td>-0.228960</td>\n",
       "      <td>-0.024250</td>\n",
       "      <td>0.046547</td>\n",
       "      <td>-0.346230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.178458</td>\n",
       "      <td>0.166055</td>\n",
       "      <td>-0.101567</td>\n",
       "      <td>0.369453</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>0.396639</td>\n",
       "      <td>-0.187978</td>\n",
       "      <td>-0.483147</td>\n",
       "      <td>0.083094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323048</td>\n",
       "      <td>-1.083814</td>\n",
       "      <td>0.049838</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>0.295810</td>\n",
       "      <td>0.135883</td>\n",
       "      <td>-0.074191</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.150527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.869017</td>\n",
       "      <td>-0.202287</td>\n",
       "      <td>-0.218739</td>\n",
       "      <td>1.496434</td>\n",
       "      <td>-0.403332</td>\n",
       "      <td>-0.013593</td>\n",
       "      <td>-0.342586</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.149568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458963</td>\n",
       "      <td>-1.058509</td>\n",
       "      <td>0.439679</td>\n",
       "      <td>-0.066668</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-1.125226</td>\n",
       "      <td>0.053537</td>\n",
       "      <td>-0.039380</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.335053</td>\n",
       "      <td>0.331464</td>\n",
       "      <td>-2.057763</td>\n",
       "      <td>-0.346175</td>\n",
       "      <td>2.583234</td>\n",
       "      <td>2.854102</td>\n",
       "      <td>-0.187547</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>-0.286614</td>\n",
       "      <td>-0.535903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191820</td>\n",
       "      <td>-0.650118</td>\n",
       "      <td>-0.114069</td>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.730073</td>\n",
       "      <td>0.383879</td>\n",
       "      <td>-0.031902</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.347171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.787763</td>\n",
       "      <td>-0.737892</td>\n",
       "      <td>-0.185794</td>\n",
       "      <td>0.362758</td>\n",
       "      <td>-0.550775</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>-0.932369</td>\n",
       "      <td>0.390445</td>\n",
       "      <td>1.349983</td>\n",
       "      <td>-0.136095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331025</td>\n",
       "      <td>1.223539</td>\n",
       "      <td>0.264791</td>\n",
       "      <td>-0.541170</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.232691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.055540</td>\n",
       "      <td>0.942471</td>\n",
       "      <td>0.986697</td>\n",
       "      <td>1.560551</td>\n",
       "      <td>-0.138755</td>\n",
       "      <td>-0.253645</td>\n",
       "      <td>0.622974</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>-0.215914</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.985031</td>\n",
       "      <td>0.204281</td>\n",
       "      <td>0.455561</td>\n",
       "      <td>-0.456576</td>\n",
       "      <td>-0.244140</td>\n",
       "      <td>-0.833487</td>\n",
       "      <td>-0.419773</td>\n",
       "      <td>-0.173549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.836500 -0.545419 -0.462979  0.537174 -0.426143 -0.100606 -0.584764   \n",
       "1 -4.289880 -2.576061 -0.092256  1.976405  2.810033 -2.669128 -0.981883   \n",
       "2  1.131318  0.139818  0.586921  1.069291 -0.334908 -0.204938 -0.135526   \n",
       "3 -0.866956  1.373947  1.948343  2.686750 -0.366790  0.568632 -0.278349   \n",
       "4 -0.842670  1.401843  0.927235  1.070402  0.843883  0.467333  0.366716   \n",
       "5  1.178458  0.166055 -0.101567  0.369453  0.017198 -0.722891  0.396639   \n",
       "6  1.869017 -0.202287 -0.218739  1.496434 -0.403332 -0.013593 -0.342586   \n",
       "7  1.335053  0.331464 -2.057763 -0.346175  2.583234  2.854102 -0.187547   \n",
       "8  1.787763 -0.737892 -0.185794  0.362758 -0.550775  0.676564 -0.932369   \n",
       "9 -1.055540  0.942471  0.986697  1.560551 -0.138755 -0.253645  0.622974   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0 -0.103956  2.268429 -0.365185  ...    0.085111  0.410736  0.137625   \n",
       "1 -0.470310 -0.025692  0.099528  ...   -0.473240 -0.307295 -2.789549   \n",
       "2  0.043821 -0.121117  0.182139  ...   -0.028126 -0.167062 -0.048054   \n",
       "3  0.739536 -1.655955  0.708396  ...    0.022719 -0.070619 -0.080307   \n",
       "4  0.616739 -1.586963  0.000041  ...    0.036573 -0.182581 -0.226834   \n",
       "5 -0.187978 -0.483147  0.083094  ...   -0.323048 -1.083814  0.049838   \n",
       "6  0.129402  0.911017  0.149568  ...   -0.458963 -1.058509  0.439679   \n",
       "7  0.685154 -0.286614 -0.535903  ...   -0.191820 -0.650118 -0.114069   \n",
       "8  0.390445  1.349983 -0.136095  ...    0.331025  1.223539  0.264791   \n",
       "9 -0.321826 -0.215914  0.816454  ...    0.153985  0.985031  0.204281   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  Class  \n",
       "0  0.602906 -0.350260  0.464407 -0.070917 -0.030486  0.049882      0  \n",
       "1  0.578976 -0.837979  0.372843  0.353451 -1.662202 -0.347171      0  \n",
       "2 -0.009912  0.417694 -0.479793  0.024360  0.023878 -0.208963      0  \n",
       "3  0.000816  0.092167  0.159131  0.157940 -0.014370 -0.253595      0  \n",
       "4 -1.029794 -0.118762 -0.228960 -0.024250  0.046547 -0.346230      0  \n",
       "5 -0.002872  0.295810  0.135883 -0.074191  0.004364 -0.150527      0  \n",
       "6 -0.066668 -0.376792 -1.125226  0.053537 -0.039380 -0.305050      0  \n",
       "7  0.915936  0.730073  0.383879 -0.031902  0.029849 -0.347171      0  \n",
       "8 -0.541170 -0.571339  0.812785  0.017984 -0.054691 -0.232691      0  \n",
       "9  0.455561 -0.456576 -0.244140 -0.833487 -0.419773 -0.173549      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at a sample of records\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    99508\n",
      "1      492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9e7/A+BW2SWyijhRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x176c650b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud transactions in the data\n",
    "count_classes = pd.value_counts(data['Class'], sort = True)\n",
    "print(count_classes)\n",
    "\n",
    "#Drawing a barplot\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Converting data to array\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 30)\n",
      "(20000, 30)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into train and test and observing their dimensions\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([79591,   409], dtype=int64))\n",
      "(array([0., 1.]), array([19917,    83], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the fraud and non-fraud records in train\n",
    "print(np.unique(X_train[:,29],return_counts=True))\n",
    "print(np.unique(X_test[:,29],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79591, 29)\n"
     ]
    }
   ],
   "source": [
    "#Now consider only the non-fraud records for training\n",
    "X_train_NF = X_train[X_train[:,-1] == 0]\n",
    "X_train_NF = X_train_NF[:,:-1]\n",
    "print(X_train_NF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Separating out the fraud records from the train \n",
    "X_train_F = X_train[X_train[:,-1] == 1]\n",
    "print(X_train_F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20409, 30)\n"
     ]
    }
   ],
   "source": [
    "#Adding/concatenating the fraud records from train data to the test\n",
    "X_test=np.concatenate((X_test,X_train_F),axis=0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test,X_eval = train_test_split(X_test, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(4082, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating the independent and the class variable\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16327, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expanding the dimensions of y for later concatenation\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train_NF.shape[1]\n",
    "encoding_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "autoencoder.add(Dense(input_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bharg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.6953 - mean_squared_error: 0.6953 - val_loss: 0.3735 - val_mean_squared_error: 0.3735\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.4200 - mean_squared_error: 0.4200 - val_loss: 0.2723 - val_mean_squared_error: 0.2723\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3728 - mean_squared_error: 0.3728 - val_loss: 0.2464 - val_mean_squared_error: 0.2464\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3636 - mean_squared_error: 0.3636 - val_loss: 0.2353 - val_mean_squared_error: 0.2353\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.2352 - val_mean_squared_error: 0.2352\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - val_loss: 0.2317 - val_mean_squared_error: 0.2317\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3495 - mean_squared_error: 0.3495 - val_loss: 0.2301 - val_mean_squared_error: 0.2301\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3506 - mean_squared_error: 0.3506 - val_loss: 0.2336 - val_mean_squared_error: 0.2336\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3500 - mean_squared_error: 0.3500 - val_loss: 0.2414 - val_mean_squared_error: 0.2414\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3442 - mean_squared_error: 0.3442 - val_loss: 0.2354 - val_mean_squared_error: 0.2354\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3462 - mean_squared_error: 0.3462 - val_loss: 0.2273 - val_mean_squared_error: 0.2273\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3469 - mean_squared_error: 0.3469 - val_loss: 0.2314 - val_mean_squared_error: 0.2314\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3473 - mean_squared_error: 0.3473 - val_loss: 0.2287 - val_mean_squared_error: 0.2287\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3446 - mean_squared_error: 0.3446 - val_loss: 0.2293 - val_mean_squared_error: 0.2293\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - val_loss: 0.2307 - val_mean_squared_error: 0.2307\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3488 - mean_squared_error: 0.3488 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3418 - mean_squared_error: 0.3418 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3406 - mean_squared_error: 0.3406 - val_loss: 0.2273 - val_mean_squared_error: 0.2273\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3417 - mean_squared_error: 0.3417 - val_loss: 0.2266 - val_mean_squared_error: 0.2266\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3437 - mean_squared_error: 0.3437 - val_loss: 0.2245 - val_mean_squared_error: 0.2245\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3396 - mean_squared_error: 0.3396 - val_loss: 0.2261 - val_mean_squared_error: 0.2261\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3405 - mean_squared_error: 0.3405 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3436 - mean_squared_error: 0.3436 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3413 - mean_squared_error: 0.3413 - val_loss: 0.2280 - val_mean_squared_error: 0.2280\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3422 - mean_squared_error: 0.3422 - val_loss: 0.2249 - val_mean_squared_error: 0.2249\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3382 - mean_squared_error: 0.3382 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3404 - mean_squared_error: 0.3404 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3365 - mean_squared_error: 0.3365 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3358 - mean_squared_error: 0.3358 - val_loss: 0.2282 - val_mean_squared_error: 0.2282\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3376 - mean_squared_error: 0.3376 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3384 - mean_squared_error: 0.3384 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3375 - mean_squared_error: 0.3375 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3384 - mean_squared_error: 0.3384 - val_loss: 0.2264 - val_mean_squared_error: 0.2264\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2251 - val_mean_squared_error: 0.2251\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3346 - mean_squared_error: 0.3346 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2235 - val_mean_squared_error: 0.2235\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3334 - mean_squared_error: 0.3334 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3361 - mean_squared_error: 0.3361 - val_loss: 0.2298 - val_mean_squared_error: 0.2298\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3345 - mean_squared_error: 0.3345 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3368 - mean_squared_error: 0.3368 - val_loss: 0.2264 - val_mean_squared_error: 0.2264\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3391 - mean_squared_error: 0.3391 - val_loss: 0.2302 - val_mean_squared_error: 0.2302\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3350 - mean_squared_error: 0.3350 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3348 - mean_squared_error: 0.3348 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3359 - mean_squared_error: 0.3359 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3362 - mean_squared_error: 0.3362 - val_loss: 0.2237 - val_mean_squared_error: 0.2237\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3376 - mean_squared_error: 0.3376 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3347 - mean_squared_error: 0.3347 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3325 - mean_squared_error: 0.3325 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3351 - mean_squared_error: 0.3351 - val_loss: 0.2266 - val_mean_squared_error: 0.2266\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3355 - mean_squared_error: 0.3355 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3299 - mean_squared_error: 0.3299 - val_loss: 0.2249 - val_mean_squared_error: 0.2249\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3321 - mean_squared_error: 0.3321 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3314 - mean_squared_error: 0.3314 - val_loss: 0.2257 - val_mean_squared_error: 0.2257\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.2270 - val_mean_squared_error: 0.2270\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3334 - mean_squared_error: 0.3334 - val_loss: 0.2282 - val_mean_squared_error: 0.2282\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3323 - mean_squared_error: 0.3323 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3332 - mean_squared_error: 0.3332 - val_loss: 0.2300 - val_mean_squared_error: 0.2300\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3324 - mean_squared_error: 0.3324 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.2268 - val_mean_squared_error: 0.2268\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3339 - mean_squared_error: 0.3339 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3327 - mean_squared_error: 0.3327 - val_loss: 0.2240 - val_mean_squared_error: 0.2240\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3333 - mean_squared_error: 0.3333 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3305 - mean_squared_error: 0.3305 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2268 - val_mean_squared_error: 0.2268\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3302 - mean_squared_error: 0.3302 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3293 - mean_squared_error: 0.3293 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3281 - mean_squared_error: 0.3281 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2243 - val_mean_squared_error: 0.2243\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3295 - mean_squared_error: 0.3295 - val_loss: 0.2262 - val_mean_squared_error: 0.2262\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3283 - mean_squared_error: 0.3283 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2238 - val_mean_squared_error: 0.2238\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55713/55713 [==============================] - 2s - loss: 0.3322 - mean_squared_error: 0.3322 - val_loss: 0.2246 - val_mean_squared_error: 0.2246\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3260 - mean_squared_error: 0.3260 - val_loss: 0.2225 - val_mean_squared_error: 0.2225\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3269 - mean_squared_error: 0.3269 - val_loss: 0.2227 - val_mean_squared_error: 0.2227\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3258 - mean_squared_error: 0.3258 - val_loss: 0.2224 - val_mean_squared_error: 0.2224\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3294 - mean_squared_error: 0.3294 - val_loss: 0.2244 - val_mean_squared_error: 0.2244\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3271 - mean_squared_error: 0.3271 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3256 - mean_squared_error: 0.3256 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3291 - mean_squared_error: 0.3291 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3329 - mean_squared_error: 0.3329 - val_loss: 0.2261 - val_mean_squared_error: 0.2261\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3297 - mean_squared_error: 0.3297 - val_loss: 0.2239 - val_mean_squared_error: 0.2239\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3278 - mean_squared_error: 0.3278 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3270 - mean_squared_error: 0.3270 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3273 - mean_squared_error: 0.3273 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3288 - mean_squared_error: 0.3288 - val_loss: 0.2256 - val_mean_squared_error: 0.2256\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3301 - mean_squared_error: 0.3301 - val_loss: 0.2254 - val_mean_squared_error: 0.2254\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3295 - mean_squared_error: 0.3295 - val_loss: 0.2250 - val_mean_squared_error: 0.2250\n",
      "Train on 55713 samples, validate on 23878 samples\n",
      "Epoch 1/1\n",
      "55713/55713 [==============================] - 2s - loss: 0.3259 - mean_squared_error: 0.3259 - val_loss: 0.2248 - val_mean_squared_error: 0.2248\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train_NF, X_train_NF,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                     validation_split=0.3,\n",
    "                    verbose=1).history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': [0.695324907640844],\n",
       "  'mean_squared_error': [0.695324907640844],\n",
       "  'val_loss': [0.37349971829447],\n",
       "  'val_mean_squared_error': [0.37349971829447]},\n",
       " {'loss': [0.42003668715649006],\n",
       "  'mean_squared_error': [0.42003668715649006],\n",
       "  'val_loss': [0.2723358369654163],\n",
       "  'val_mean_squared_error': [0.2723358369654163]},\n",
       " {'loss': [0.3727933886126581],\n",
       "  'mean_squared_error': [0.3727933886126581],\n",
       "  'val_loss': [0.2464360946572369],\n",
       "  'val_mean_squared_error': [0.2464360946572369]},\n",
       " {'loss': [0.3635741629834747],\n",
       "  'mean_squared_error': [0.3635741629834747],\n",
       "  'val_loss': [0.23530568360386206],\n",
       "  'val_mean_squared_error': [0.23530568360386206]},\n",
       " {'loss': [0.35717580086608725],\n",
       "  'mean_squared_error': [0.35717580086608725],\n",
       "  'val_loss': [0.23517274355077597],\n",
       "  'val_mean_squared_error': [0.23517274355077597]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4432399e+00,  8.7036669e-01,  1.6521544e+00,  8.1679034e-01,\n",
       "        -7.3549169e-01,  3.2794514e-01, -5.8283901e-01,  1.2259694e+00,\n",
       "         1.9634995e-01, -1.0537603e+00, -1.3853183e+00,  2.9447156e-01,\n",
       "        -8.4814215e-01, -1.4758524e-01, -1.6575549e+00, -1.6407430e-01,\n",
       "         4.4036913e-01,  9.2143387e-02,  7.4829608e-03,  1.2122378e-01,\n",
       "         9.5195964e-02, -5.8713667e-02, -8.5580945e-03, -7.3304877e-02,\n",
       "        -3.8180023e-02,  1.3117905e-02, -7.5090211e-03,  1.8448427e-02,\n",
       "        -1.4113742e-01],\n",
       "       [ 1.1887827e+00, -1.9675851e-01, -4.7158360e-02, -4.5041168e-01,\n",
       "        -5.8828139e-01, -1.3243284e+00, -3.6417242e-02, -2.4616992e-01,\n",
       "        -1.1154726e+00,  5.6852961e-01,  3.2103717e-01, -1.5510117e-01,\n",
       "         3.5319638e-01, -1.9294584e-01,  3.2146811e-01,  9.6328020e-02,\n",
       "         1.5686655e-01,  4.4948488e-02,  3.6132753e-02, -5.3251833e-02,\n",
       "        -1.6575530e-03, -1.2422707e-03, -6.1567463e-03, -1.0330281e-01,\n",
       "        -1.4341621e-02, -2.1778617e-02, -3.2922309e-02, -1.8280478e-02,\n",
       "        -2.0252237e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making predictions on the train data\n",
    "predictions=autoencoder.predict(X_train_NF)\n",
    "\n",
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16327, 30)\n",
      "(15942, 30)\n",
      "(385, 30)\n"
     ]
    }
   ],
   "source": [
    "##We want to separate out fraud records and non-fraud records for later use\n",
    "f = np.hstack((X_test,y_test))\n",
    "print(f.shape)\n",
    "\n",
    "test_nf=f[f[:,29]==0]\n",
    "print(test_nf.shape)\n",
    "\n",
    "test_f=f[f[:,29]==1]\n",
    "print(test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14304/15942 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2301862383409241, 0.2301862383409241]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the non fraud data separately \n",
    "autoencoder.evaluate(test_nf[:,:29],test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/385 [=>............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.560293519651736, 19.560293519651736]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the errors from the fraud data separately\n",
    "autoencoder.evaluate(test_f[:,:29],test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining predictions for non fraud records\n",
    "predictions_nf=autoencoder.predict(test_nf[:,:29])\n",
    "\n",
    "#Obtaining predictions for fraud records\n",
    "predictions_f=autoencoder.predict(test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2301862366657775"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the error computation method by autoencoder(Mean Squared Error). The computation is as follows \n",
    "np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30963621, 0.05360371, 0.19483006, 0.07043692, 0.40740092])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing errors on the non-fraud data\n",
    "errors_nf = np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)), axis=1)\n",
    "errors_nf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89.90100995, 75.02913094, 17.7202158 ,  4.01550371, 71.16703302])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing errors on the fraud data\n",
    "errors_f = np.mean(np.square(np.abs(test_f[:,:29]-predictions_f)), axis=1)\n",
    "errors_f[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016708967105815792\n",
      "110.20634003428978\n",
      "0.12419686431327694\n",
      "0.046207987150603376\n",
      "138.1402238710232\n",
      "7.99036979283013\n"
     ]
    }
   ],
   "source": [
    "#Computing the distribution of errors in both non-fraud and fraud data\n",
    "print(np.min(errors_nf))\n",
    "print(np.max(errors_nf))\n",
    "print(np.median(errors_nf))\n",
    "\n",
    "print(np.min(errors_f))\n",
    "print(np.max(errors_f))\n",
    "print(np.median(errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x176cbbf6550>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x176cbbfd908>,\n",
       "  <matplotlib.lines.Line2D at 0x176cbbfdf28>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x176cbc09ba8>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x176cbc09080>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x176cbbf6c88>,\n",
       "  <matplotlib.lines.Line2D at 0x176cbbf6da0>]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x176c669ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLotting the error box plots \n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.boxplot(errors_f)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(errors_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "192\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7971"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experimentation to fix a threshold for classification of a transaction into fraud or non-fraud\n",
    "print(sum(errors_nf>np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_nf)))\n",
    "sum(errors_nf>np.median(errors_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942,)\n",
      "(385,)\n"
     ]
    }
   ],
   "source": [
    "print(errors_nf.shape)\n",
    "print(errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15942, 29)\n",
      "(385, 29)\n"
     ]
    }
   ],
   "source": [
    "print(predictions_nf.shape)\n",
    "print(predictions_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = autoencoder.predict(X_test[:,:29])\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "train_pred = autoencoder.predict(X_train_NF[:,:29])\n",
    "mean_recon = (((train_pred - X_train_NF)**2).mean(-1).mean())\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "scores_f1 = []\n",
    "thres = []\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    scores_f1.append(f1_score(y_test,fraud))\n",
    "    thres.append(th+mean_recon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16327, 29)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(thres, scores_f1)\n",
    "\n",
    "print(thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "fraud = (test_recon>thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "confusion_matrix(y_test, fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Predicting on Valdation \n",
    "\n",
    "predictions_eval=autoencoder.predict(X_eval[:,:29])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors_eval=np.square(np.subtract(predictions_eval,X_eval[:,:29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraud_eval=(((errors_eval-X_eval[:,:29])**2).mean(-1))>2.925861360803925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3682  293]\n",
      " [  15   92]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=X_eval[:,29],y_pred=fraud_eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
